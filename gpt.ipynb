{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embds, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embds, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embds, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embds, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones((T,T))))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention since query, keys and values comes from the same input x\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,head_size\n",
    "        q = self.query(x) # B,T,head_size\n",
    "        v = self.value(x) # B,T,head_size\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size**-0.5 # (B,T,head_size) @ (B,head_size,T) = (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        \n",
    "        return wei @ v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, n_embds, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embds, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embds)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"Just a MLP\"\"\"\n",
    "    def __init__(self, n_embds):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embds, n_embds),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embds, n_embds) # works as a projection layer\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embds, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embds // n_heads\n",
    "        self.attn = MultiHead(n_heads, n_embds, head_size)\n",
    "        self.ff = FeedFoward(n_embds)\n",
    "        self.ln1 = nn.LayerNorm(n_embds)\n",
    "        self.ln2 = nn.LayerNorm(n_embds)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note the residual connection\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embds, n_heads, head_size, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embds)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embds)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embds, n_heads),\n",
    "            Block(n_embds, n_heads),\n",
    "            Block(n_embds, n_heads),\n",
    "            nn.LayerNorm(n_embds),\n",
    "        )\n",
    "        self.proj = nn.Linear(n_embds, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_emb(idx) # B,T,n_embds\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device)) # T,n_embds\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.proj(x)\n",
    "        return logits\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
